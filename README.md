# AI Commercial Agent â€” Multi-Provider LLM Architecture with Fallback
## ğŸ“Œ Overview

Proyecto backend que implementa un **agente LLM desacoplado con ejecuciÃ³n de herramientas (tool calling)** y una arquitectura resiliente basada en mÃºltiples proveedores de modelo con:

* Retry automÃ¡tico por proveedor

* Fallback en cascada

* Orden configurable vÃ­a .env

* Logging estructurado

* MÃ©tricas simples de uso por proveedor

* Memoria conversacional

* EjecuciÃ³n dinÃ¡mica de herramientas backend

El sistema estÃ¡ diseÃ±ado con enfoque **production-aware**, priorizando:

* Desacoplamiento

* Resiliencia

* ConfiguraciÃ³n runtime

* Extensibilidad


## ğŸ— Arquitectura General

```
Client
  â†“
FastAPI (main.py)
  â†“
AgentCore
  â†“
LLMRouter
  â†“
[ HuggingFaceLLM | MockLLM | (otros futuros) ]
  â†“
Tool Executor (si aplica)
  â†“
Business Logic (SIMULATED DB)
```

## ğŸ§  Componentes Clave

## 1ï¸âƒ£ AgentCore

Responsable de:

* ConstrucciÃ³n de prompts

* GestiÃ³n de memoria conversacional

* InvocaciÃ³n del LLM (agnÃ³stico al proveedor)

* DetecciÃ³n robusta de JSON para tool-calling

* EjecuciÃ³n de herramientas vÃ­a tool_executor

El agente no sabe quÃ© modelo se usa. Solo conoce la interfaz comÃºn.

## 2ï¸âƒ£ LLMInterface

Define el contrato comÃºn:

```
class LLMInterface(ABC):
    @abstractmethod
    def generate(self, system_prompt: str, user_prompt: str) -> str:
        pass
```

Cualquier proveedor debe implementar este mÃ©todo.

Esto permite aplicar el principio de:

>InversiÃ³n de dependencias (Dependency Inversion Principle)

## 3ï¸âƒ£ LLMRouter (Resiliencia y OrquestaciÃ³n)

El LLMRouter implementa:

* Orden configurable vÃ­a .env

* Retry automÃ¡tico por proveedor

* Fallback automÃ¡tico

* MÃ©tricas simples de uso

* Logging estructurado

Variables configurables:

```
LLM_ORDER=hf,mock
LLM_MAX_RETRIES=2
```

Ejemplo de flujo:

```
Intento 1 â†’ HuggingFace
Intento 2 â†’ HuggingFace
Si falla â†’ pasa a MockLLM
```

## 4ï¸âƒ£ Tool Execution

Si el modelo responde en formato JSON vÃ¡lido:

```JSON
{
  "tool": "get_student_status",
  "arguments": {
    "student_id": "123"
  }
}
```

El sistema:

**1.** Detecta el JSON con regex robusta

**2.** Parsea de forma segura

**3.** Ejecuta la herramienta correspondiente

**4.** Devuelve el resultado formateado

Esto simula un patrÃ³n real de:

> LLM como orquestador + backend determinÃ­stico

## ğŸ”„ Â¿Por quÃ© Multi-Provider en una empresa?

En entornos reales:

* Los proveedores pueden fallar

* Hay lÃ­mites de cuota

* Puede haber latencia alta

* Puede requerirse cambio de proveedor por costo

* Puede requerirse modelo local por compliance

Esta arquitectura permite:

* Cambiar proveedor sin tocar el core

* Agregar fallback por resiliencia

* Probar modelos distintos en staging

* Hacer migraciones graduales

Ejemplo real:

```
ProducciÃ³n â†’ OpenAI
Fallback â†’ Azure
Emergencia â†’ Modelo local
```

Sin modificar el agente.

## â• CÃ³mo Agregar un Nuevo LLM

Supongamos que quieres agregar **OpenAI**.

### Paso 1 â€” Crear el proveedor

Crear archivo:

```
app/agent/openai_llm.py
```

Implementar:

```Python
from app.agent.llm_interface import LLMInterface

class OpenAILLM(LLMInterface):

    def generate(self, system_prompt: str, user_prompt: str) -> str:
        # ImplementaciÃ³n real usando SDK de OpenAI
        return "respuesta del modelo"
```

### Paso 2 â€” Registrar en AgentCore

Modificar el constructor:

```Python
self.llm = LLMRouter({
    "hf": HuggingFaceLLM(),
    "mock": MockLLM(),
    "openai": OpenAILLM()
})
```

### Paso 3 â€” Configurar orden en .env

```
LLM_ORDER=openai,hf,mock
```

Listo.

* No se toca:

* Tool execution

* Memory

* Parsing

* API

* Router interno

## ğŸ”§ CÃ³mo Cambiar de Modelo en ProducciÃ³n

Solo cambiar en **.env** :
```
LLM_ORDER=openai
```
O:
```
LLM_ORDER=openai,hf
```

Reiniciar servicio.

Arquitectura completamente desacoplada.

## ğŸ“Š MÃ©tricas Internas

El router mantiene estadÃ­sticas simples:
```
router.get_stats()
```

Ejemplo:

```
{
  "hf": 12,
  "mock": 3
}
```

Permite detectar:

* QuÃ© proveedor se usa mÃ¡s

* Si uno estÃ¡ fallando frecuentemente

## ğŸ›  TecnologÃ­as Utilizadas

- Python 3.10+
- FastAPI
- httpx (LLM API client)
- Arquitectura basada en interfaces (Strategy Pattern)
- Logging estructurado (logging module)
- ConfiguraciÃ³n por variables de entorno (.env)

## ğŸ¯ QuÃ© Demuestra Este Proyecto

Este proyecto demuestra:

* DiseÃ±o desacoplado

* Principios SOLID

* Resiliencia ante fallos externos

* Tool-calling robusto

* Arquitectura extensible

* ConfiguraciÃ³n runtime

* Pensamiento orientado a producciÃ³n

No se trata de un chatbot conversacional genÃ©rico.

Es un **orquestador LLM con backend determinÃ­stico y fallback resiliente**.

## ğŸ“¸ Screenshots

## ğŸ”§ Ejemplo de EjecuciÃ³n de Herramientas

El agente detecta la intenciÃ³n, llama a la herramienta apropiada e inyecta el resultado en la respuesta.

### - Log de ejecuciÃ³n de herramienta

![EjecuciÃ³n de Herramienta](docs/images/tool-execution-log.PNG)

### - BÃºsqueda exitosa de estudiante

![Estudiante 1024](docs/images/tool-response-1024.PNG)

### - Caso de estudiante inexistente

![Estudiante 9999](docs/images/tool-response-9999.PNG)

### ğŸ¤– Prueba de No Chatbot GenÃ©rico

El agente NO responde como un chatbot conversacional libre. Solo utiliza herramientas cuando corresponde y se limita a su dominio especÃ­fico:

### - Pregunta fuera de dominio

![Prueba No Chatbot](docs/images/no-chatbot-test.PNG)

---

## ğŸš€ CÃ³mo Ejecutar

**1.** Crear entorno virtual.

**2.** Instalar dependencias:

```cmd
pip install -r requirements.txt
```

**3.** Configurar **.env**:

```
HF_API_TOKEN=Lorem_Ipsum_8f4h7kc95d6sfhj6l64Ã±329

LLM_ORDER=hf,mock
LLM_MAX_RETRIES=2
```

**4.** Ejecutar:

```
uvicorn app.main:app --reload
```

## ğŸ“ Estructura del Proyecto
```
ai-commercial-agent/
â”‚
â”œâ”€â”€ app/
â”‚ â”œâ”€â”€ agent/
â”‚ â”‚ â”œâ”€â”€ agent_core.py
â”‚ â”‚ â”œâ”€â”€ hf_llm.py
â”‚ â”‚ â”œâ”€â”€ llm_interface.py
â”‚ â”‚ â”œâ”€â”€ llm_router.py
â”‚ â”‚ â”œâ”€â”€ memory.py
â”‚ â”‚ â””â”€â”€ mock_llm.py
â”‚ â”‚
â”‚ â”œâ”€â”€ rag/ # (MÃ³dulo en desarrollo)
â”‚ â”‚ â”œâ”€â”€ embeddings.py
â”‚ â”‚ â””â”€â”€ vector_store.py
â”‚ â”‚
â”‚ â”œâ”€â”€ tools/
â”‚ â”‚ â”œâ”€â”€ student_tools.py
â”‚ â”‚ â””â”€â”€ tool_executor.py
â”‚ â”‚
â”‚ â””â”€â”€ main.py
â”‚
â”œâ”€â”€ data/
â”œâ”€â”€ docs/
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```
> âš ï¸ Nota: El mÃ³dulo RAG (Retrieval-Augmented Generation) se encuentra en desarrollo y serÃ¡ activado en la rama `feature/rag-module`.  
> La versiÃ³n actual en `main` implementa arquitectura multi-provider, tool calling y fallback resiliente.

## ğŸ§© Posibles Mejoras Futuras

* Circuit breaker pattern

* MÃ©tricas exportables (Prometheus)

* Streaming de tokens

* Persistencia de memoria en base de datos

* Feature flags por proveedor

* Rate limiting por modelo

## ğŸ“Œ ConclusiÃ³n

Este proyecto implementa un agente LLM modular, resiliente y configurable, capaz de integrarse con mÃºltiples proveedores y ejecutar lÃ³gica backend determinÃ­stica, siguiendo principios de arquitectura limpia y extensibilidad empresarial.
